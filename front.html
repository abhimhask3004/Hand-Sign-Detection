<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sign Language Detection</title>
    <style>
        /* Reset default margin and padding for all elements */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Set a background color and font for the entire page */
        body {
            font-family: Arial, sans-serif;
            background-color: #f5f5f5;
        }

        /* Header styles */
        header {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 20px 0;
        }

        header h1 {
            font-size: 36px;
        }

        /* Main content styles */
        main {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        /* Video container styles */
        #video-container {
            text-align: center;
        }

        video {
            max-width: 100%;
        }

        /* Button styles */
        button {
            background-color: #333;
            color: #fff;
            border: none;
            padding: 10px 20px;
            margin: 10px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.3s;
        }

        button:hover {
            background-color: #555;
        }

        /* Output container styles */
        #output-container {
            margin-top: 20px;
        }

        h2 {
            font-size: 24px;
        }

        p#sign-output {
            font-size: 18px;
        }

        textarea#sentence-output {
            width: 100%;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ccc;
            border-radius: 4px;
            resize: none;
        }

        /* Footer styles */
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
        }

        /* Media query for responsiveness */
        @media (max-width: 600px) {
            main {
                padding: 10px;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Sign Language Detection</h1>
    </header>
    <main>
        <section id="video-container">
            <video id="video-feed" width="640" height="480" autoplay></video>
            <button id="start-button">Start Camera</button>
            <button id="stop-button">Stop Camera</button>
        </section>
        <section id="output-container">
            <h2>Detected Sign:</h2>
            <p id="sign-output">No sign detected</p>
            <h2>Detected Sentence:</h2>
            <textarea id="sentence-output" rows="4" cols="50" readonly></textarea>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 Your Name</p>
    </footer>
    <script>
        // Your JavaScript code for camera access, sign language detection, and sentence formation goes here
               import math

import cv2

from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
    cap = cv2.VideoCapture(0)
    detector = HandDetector(maxHands=1)
    classifier = Classifier("model/keras_model.h5", "model/labels.txt")
    offset = 20
    imgSize = 300
    # folder="data/C"
    counter = 0
    labels = ["A", "B", "C", "D"]
    ans =""
    while True:
        success, img = cap.read()
        imgOutput = img.copy()
        hands, img = detector.findHands(img)
        const video = document.getElementById("video-feed");
        const startButton = document.getElementById("start-button");
        const stopButton = document.getElementById("stop-button");
        const signOutput = document.getElementById("sign-output");
        const sentenceOutput = document.getElementById("sentence-output");

        // JavaScript code for starting and stopping the camera
        let stream = null;
        startButton.addEventListener("click", async () => {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (error) {
                console.error("Error accessing the camera: ", error);
            }
        });
        stopButton.addEventListener("click", () => {
            if (stream) {
                const tracks = stream.getTracks();
                tracks.forEach(track => track.stop());
                video.srcObject = null;
            }
        });

        // Your sign language detection and sentence formation code goes here
        import math

import cv2

from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
    cap = cv2.VideoCapture(0)
    detector = HandDetector(maxHands=1)
    classifier = Classifier("model/keras_model.h5", "model/labels.txt")
    offset = 20
    imgSize = 300
    # folder="data/C"
    counter = 0
    labels = ["A", "B", "C", "D"]
    ans =""
    while True:
        success, img = cap.read()
        imgOutput = img.copy()
        hands, img = detector.findHands(img)
        if hands:
            hand = hands[0]
            x, y, w, h = hand['bbox']
            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]
            imgCropShape = imgCrop.shape
            aspectRatio = h / w
            if aspectRatio > 1:
                k = imgSize / h
                wCal = math.ceil(k * w)
                imgResize = cv2.resize(imgCrop, (wCal, imgSize))
                imgResizeShape = imgResize.shape
                wGap = math.ceil((imgSize - wCal) / 2)
                imgWhite[:, wGap:wCal + wGap] = imgResize
                prediction, index = classifier.getPrediction(imgWhite, draw=False)
                #output.append(labels[index])

                cv2.rectangle(imgOutput, (x - offset, y - offset - 50), (x - offset + 90, y - offset),
                (255, 0, 255), cv2.FILLED)
                cv2.putText(imgOutput,labels[index], (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7,
                (255, 255, 255),
                 2)
                cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset),
                (255, 0, 255),
                4)
                ans = "".join(labels[index])
                print(ans)
            else:
                k = imgSize / w
                hCal = math.ceil(k * h)
                imgResize = cv2.resize(imgCrop, (imgSize, hCal))
                imgResizeShape = imgResize.shape
                hGap = math.ceil((imgSize - hCal) / 2)
                imgWhite[hGap:hCal + hGap, :] = imgResize
                prediction, index = classifier.getPrediction(imgWhite, draw=False)
               # output.append(labels[index])
               # output="".join(labels[index])
                cv2.rectangle(imgOutput, (x - offset, y - offset - 50), (x - offset + 90, y - offset),
                (255, 0, 255), cv2.FILLED)
                cv2.putText(imgOutput, labels[index], (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7,
                (255, 255, 255),
                2)
                cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset),
                (255, 0, 255),
                4)
                ans = "".join(labels[index])
                print(ans)

            # cv2.imshow("imagecrop", imgCrop)
            # cv2.imshow("imageWhite", imgWhite)
            cv2.imshow("image", imgOutput)

        // You would need to integrate machine learning and computer vision libraries for this.

        // Example for updating detected sign and sentence
        function updateDetectedSign(sign, sentence) {
            signOutput.textContent = Detected Sign: ${sign};
            sentenceOutput.value = Detected Sentence: ${sentence};
        }
    </script>
</body>
</html>